{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb8df691-a2dc-493b-aba0-82dc5d7c562b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pexpect\n",
    "import json\n",
    "import os\n",
    "\n",
    "class LeanServer:\n",
    "    def __init__(self):\n",
    "        # Get the path where you download repl\n",
    "        path_to_repl = os.environ.get('PATH_TO_LEAN_REPL')\n",
    "\n",
    "        # Run the command\n",
    "        self.proc = pexpect.spawn(\n",
    "            \"lake env lean --run REPL/Main.lean\", \n",
    "            cwd=path_to_repl, \n",
    "            encoding=\"utf-8\")\n",
    "\n",
    "    def run_code(self, code, env=None, verbose=False):\n",
    "        if env:\n",
    "            command = (\n",
    "                json.dumps(dict(cmd=code, env=env))\n",
    "            )  # [1:-1] removes single quotes\n",
    "        else:\n",
    "            command = (\n",
    "                '{ \"cmd\" : \"' + repr(code)[1:-1] + '\" }'\n",
    "            )  # [1:-1] removes single quotes\n",
    "\n",
    "        if verbose: print(command)\n",
    "        self.proc.sendline(command)\n",
    "        self.proc.expect_exact(command + \"\\r\\n\")\n",
    "        self.proc.sendline()\n",
    "        self.proc.expect_exact(\"\\r\\n\")\n",
    "        try:\n",
    "            index = self.proc.expect('env\": \\d+\\}', timeout=20)\n",
    "            output = self.proc.before + self.proc.match.group()\n",
    "            if verbose: print(output)\n",
    "            return json.loads(output)\n",
    "            \n",
    "        except pexpect.exceptions.TIMEOUT:\n",
    "            raise pexpect.exceptions.TIMEOUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "402d3cc3-8116-4251-82af-c201828ade8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_goal(state):\n",
    "    goal = None\n",
    "    for msg in state['messages']:\n",
    "        if msg['data'].startswith('unsolved goals\\n'):\n",
    "            goal = '\\n'.join(msg['data'].split('\\n')[1:])\n",
    "            \n",
    "        elif msg['severity'] == 'error':\n",
    "            return None\n",
    "            \n",
    "    return goal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "88aff439-9a9c-4cd3-a69e-f0872fb76d1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'env': 0,\n",
      " 'messages': [{'data': 'unsolved goals\\n'\n",
      "                       'm n : ℕ\\n'\n",
      "                       'h : Nat.Coprime m n\\n'\n",
      "                       '⊢ Nat.gcd m n = 1',\n",
      "               'endPos': {'column': 69, 'line': 4},\n",
      "               'pos': {'column': 68, 'line': 4},\n",
      "               'severity': 'error'}]}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "code = \"\"\"\n",
    "import Mathlib.Data.Nat.Prime\n",
    "\n",
    "theorem test_thm (m n : Nat) (h : m.Coprime n) : m.gcd n = 1 := by {}\n",
    "\"\"\"\n",
    "\n",
    "lean = LeanServer()\n",
    "state = lean.run_code(code)\n",
    "lean.proc.close()\n",
    "pprint(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d4565a45-c4c7-4f2b-8a34-510e3d58f78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code = \"\"\"\n",
    "# import Mathlib.Data.Nat.Prime\n",
    "\n",
    "# theorem test_thm (m n : Nat) (h : m.Coprime n) : m.gcd n = 1 := by \n",
    "\n",
    "# rw [← h.gcd_eq_one]\n",
    "# \"\"\"\n",
    "\n",
    "# lean = LeanServer()\n",
    "# state = lean.run_code(code)\n",
    "# lean.proc.close()\n",
    "\n",
    "# pprint(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "72bfb67a-c487-408a-93f3-b4429a4e3ee2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not state.get('sorries') and not state.get('messages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2e628d14-c34d-4cc6-9d33-69795be69fec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# Load model and tokenizer\n",
    "import os\n",
    "import transformers\n",
    "\n",
    "model_name = 'wellecks/llmstep-mathlib4-pythia2.8b'\n",
    "model = transformers.GPTNeoXForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "tokenizer = transformers.GPTNeoXTokenizerFast.from_pretrained(model_name)\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'true'  # prevents an annoying warning\n",
    "\n",
    "def generate(prompt):\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    out = model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=256,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    text = tokenizer.decode(out[0][input_ids.shape[1]:], skip_special_tokens=True)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "34079012-3327-4d1d-98e4-87ed4c9e0b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GOAL]m n : ℕ\n",
      "h : Nat.Coprime m n\n",
      "⊢ Nat.gcd m n = 1[PROOFSTEP]\n"
     ]
    }
   ],
   "source": [
    "# Generate a next step\n",
    "prompt = f\"[GOAL]{get_goal(state)}[PROOFSTEP]\"\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "848800fe-5173-4a24-abd5-7654385e1e1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rw [← gcd_eq_gcd_ab, gcd_comm]\n"
     ]
    }
   ],
   "source": [
    "next_step = generate(prompt)\n",
    "print(next_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "406d8ec3-becd-4986-804c-b7bf696b68ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'env': 0,\n",
      " 'messages': [{'data': \"unknown identifier 'gcd_eq_gcd_ab'\",\n",
      "               'endPos': {'column': 19, 'line': 6},\n",
      "               'pos': {'column': 6, 'line': 6},\n",
      "               'severity': 'error'},\n",
      "              {'data': \"tactic 'rewrite' failed, equality or iff proof \"\n",
      "                       'expected\\n'\n",
      "                       '  ?m.45\\n'\n",
      "                       'm n : ℕ\\n'\n",
      "                       'h : Nat.Coprime m n\\n'\n",
      "                       '⊢ Nat.gcd m n = 1',\n",
      "               'endPos': {'column': 19, 'line': 6},\n",
      "               'pos': {'column': 4, 'line': 6},\n",
      "               'severity': 'error'}]}\n"
     ]
    }
   ],
   "source": [
    "code = \"\"\"\n",
    "import Mathlib.Data.Nat.Prime\n",
    "\n",
    "theorem test_thm (m n : Nat) (h : m.Coprime n) : m.gcd n = 1 := by \n",
    "\n",
    "\"\"\" + next_step\n",
    "\n",
    "lean = LeanServer()\n",
    "state = lean.run_code(code)\n",
    "lean.proc.close()\n",
    "\n",
    "pprint(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "20b48801-8f4b-4e50-8fcb-f047f0ebc138",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Reference: https://github.com/wellecks/ntptutorial/blob/main/partI_nextstep/ntp_python/proofsearch_pylean.py\n",
    "\"\"\"\n",
    "\n",
    "# Utilities for interacting with Lean and proof search\n",
    "\n",
    "from pylean import LeanServer\n",
    "import torch\n",
    "import heapq\n",
    "import concurrent\n",
    "import transformers\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from typing import List, Tuple\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'  \n",
    "\n",
    "\n",
    "def is_done(state):\n",
    "    return not state.get('sorries') and not state.get('messages')\n",
    "\n",
    "\n",
    "def get_goal(state):\n",
    "    goal = None\n",
    "    msgs = state.get('messages')\n",
    "\n",
    "    if msgs:\n",
    "        for msg in msgs:\n",
    "            \n",
    "            if msg['data'].startswith('unsolved goals\\n'):\n",
    "                goal = '\\n'.join(msg['data'].split('\\n')[1:])\n",
    "                \n",
    "            elif msg['severity'] == 'error':\n",
    "                return None\n",
    "    return goal\n",
    "\n",
    "\n",
    "def get_errors(state):\n",
    "    return state['messages']\n",
    "\n",
    "\n",
    "def parse_step(step):\n",
    "    step = step.replace('<|endoftext|>', '')\n",
    "    return step\n",
    "\n",
    "\n",
    "def format_code(header, statement, steps_so_far, next_step):\n",
    "    return header + (statement.replace(\" {}\", \"\") + '\\n' + '\\n'.join(steps_so_far + [next_step]))\n",
    "    \n",
    "\n",
    "def run_code(code):\n",
    "    lean = LeanServer()\n",
    "    out = lean.run_code(code)\n",
    "    lean.proc.close()\n",
    "    del lean\n",
    "    return out\n",
    "\n",
    "\n",
    "def sequence_scores(out, prompt_length, model, tokenizer):\n",
    "    # Returns each output sequence's log probability normalized by the number of tokens.\n",
    "    # An output sequence is defined as the tokens after the prompt up to and including eos.\n",
    "    text = tokenizer.batch_decode(out.sequences)\n",
    "    input_ids = tokenizer(\n",
    "        text, return_tensors=\"pt\", padding='longest', truncation=True\n",
    "    ).to(model.device)\n",
    "    with torch.no_grad():\n",
    "        out = model(**input_ids)\n",
    "        probs = torch.log_softmax(out.logits, dim=-1).detach()\n",
    "        probs = probs[:, :-1, :]\n",
    "        input_ids_shifted = input_ids.input_ids[:, 1:]\n",
    "        log_probs = torch.gather(probs, 2, input_ids_shifted[:, :, None]).squeeze(-1)\n",
    "        log_probs = log_probs[:, prompt_length:]\n",
    "        up_to_eos_mask = (input_ids_shifted[:,prompt_length:].eq(\n",
    "            tokenizer.eos_token_id).cumsum(1).cumsum(1) <= 1).type(log_probs.dtype)\n",
    "        normalized_sequence_scores = (log_probs * up_to_eos_mask).sum(1) / up_to_eos_mask.sum(1)\n",
    "    return normalized_sequence_scores\n",
    "\n",
    "\n",
    "def generate(prompt, model, tokenizer, temperatures, num_samples) -> Tuple[List[str], List[float]]:\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt').to(model.device)\n",
    "    texts = []\n",
    "    scores = []\n",
    "    with torch.no_grad():\n",
    "        # Does beam search at temp 0.0, otherwise temperature sampling.\n",
    "        for temp in temperatures:\n",
    "            decoding_params = dict(\n",
    "                max_new_tokens=256,\n",
    "                do_sample=temp > 0,\n",
    "                temperature=temp,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                num_return_sequences=num_samples,\n",
    "                return_dict_in_generate=True,\n",
    "                output_scores=True,\n",
    "            )\n",
    "            if temp == 0.0:\n",
    "                decoding_params['num_beams'] = num_samples\n",
    "            out = model.generate(\n",
    "                input_ids, **decoding_params\n",
    "            )\n",
    "            \n",
    "            texts.extend(tokenizer.batch_decode(\n",
    "                out.sequences[:,input_ids.shape[1]:],\n",
    "                skip_special_tokens=True\n",
    "            ))\n",
    "            scores_ = sequence_scores(\n",
    "                out=out, \n",
    "                prompt_length=input_ids.shape[1], \n",
    "                model=model, \n",
    "                tokenizer=tokenizer\n",
    "            )\n",
    "            scores.extend(scores_.view(-1).tolist())\n",
    "\n",
    "    texts, scores = _unique_sorted(texts, scores)\n",
    "    return texts, scores\n",
    "\n",
    "\n",
    "def _unique_sorted(texts, scores):\n",
    "    texts_, scores_ = [], []\n",
    "    for t, s in sorted(zip(texts, scores), key=lambda x: -x[1]):\n",
    "        if t not in texts_:\n",
    "            texts_.append(t)\n",
    "            scores_.append(s)\n",
    "    return texts_, scores_\n",
    "\n",
    "\n",
    "def _print_type_checked_candidates(results):\n",
    "    print('--- type-checked candidates:\\n\\t' + '\\n\\t'.join(\n",
    "        '(%.3f) %s' % (step_score, step) \n",
    "        for state, step, step_score in results if (\n",
    "        get_goal(state) is not None or is_done(state))\n",
    "    ))\n",
    "\n",
    "\n",
    "def _print_current(theorem_statement, steps):\n",
    "    print('--- current:\\n\\t%s\\n\\t%s' % (\n",
    "        theorem_statement.replace('{}', ''),\n",
    "        '\\n\\t'.join(steps)) \n",
    "    )\n",
    "\n",
    "\n",
    "def best_first_search(model, tokenizer, header, statement, max_iters, \n",
    "                      temperatures, num_samples, verbose=False) -> dict:\n",
    "    \n",
    "    goal = get_goal(run_code(header + statement))\n",
    "    if goal is None:\n",
    "        return {\n",
    "            'theorem_statement': statement, \n",
    "            'success': False, \n",
    "            'msg': run_code(header + statement)\n",
    "        }\n",
    "\n",
    "    # Score, steps-so-far, goal state\n",
    "    queue = [(0.0, [], goal)]\n",
    "    visited = set()\n",
    "    while len(queue) > 0 and max_iters > 0:\n",
    "        # Dequeue the tuple with minimum score\n",
    "        score, steps, goal = heapq.heappop(queue)\n",
    "        visited.add(goal)\n",
    "        if verbose:\n",
    "            _print_current(statement, steps)\n",
    "\n",
    "        # Generate next-step candidates\n",
    "        prompt = f\"[GOAL]{goal}[PROOFSTEP]\"\n",
    "        step_cands, step_scores = generate(\n",
    "            prompt, \n",
    "            model, \n",
    "            tokenizer, \n",
    "            temperatures=temperatures, \n",
    "            num_samples=num_samples\n",
    "        )\n",
    "\n",
    "        # Run type checking in parallel via futures. \n",
    "        with ThreadPoolExecutor(max_workers=16) as executor:\n",
    "            # We need to save the step and score associated to each future.\n",
    "            future2step = {}\n",
    "            for step, step_score in zip(step_cands, step_scores):\n",
    "                code = format_code(header, statement, steps, step)\n",
    "                future = executor.submit(run_code, **dict(code=code))\n",
    "                future2step[future] = (step, step_score)\n",
    "\n",
    "            # Collect the type checking results as they complete.\n",
    "            results = []\n",
    "            for future in tqdm(concurrent.futures.as_completed(future2step.keys()), total=len(future2step)):\n",
    "                result = future.result()\n",
    "                results.append((result, *future2step[future]))\n",
    "\n",
    "        if verbose:\n",
    "            _print_type_checked_candidates(results)\n",
    "        for state, step, step_score in results:\n",
    "            # Stop if we have found a complete proof.\n",
    "            if is_done(state):\n",
    "                return {\n",
    "                    'theorem_statement': statement, \n",
    "                    'proof': steps + [step], \n",
    "                    'state': state,\n",
    "                    'score': score - step_score,\n",
    "                    'success': True\n",
    "                }\n",
    "            goal_cand = get_goal(state)\n",
    "            # Add new candidates to the queue.\n",
    "            if goal_cand is not None and goal_cand not in visited:\n",
    "                # Score is normalized negative log probability summed across steps\n",
    "                new_score = (score - step_score)\n",
    "                heapq.heappush(\n",
    "                    queue, (new_score, steps+[step], goal_cand)\n",
    "                )\n",
    "        \n",
    "        max_iters -= 1\n",
    "\n",
    "    return {'theorem_statement': statement, 'success': False}\n",
    "\n",
    "\n",
    "def _save(results):\n",
    "    from datetime import datetime\n",
    "    import json\n",
    "    now = datetime.now()\n",
    "    dt_string = now.strftime(\"%d-%m-%Y-%H-%M-%S\")\n",
    "    output_file = 'results__%s.json' % (dt_string)\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(results, f, indent=4)\n",
    "        print(output_file)\n",
    "\n",
    "\n",
    "def load_model(model_name):\n",
    "    model = transformers.GPTNeoXForCausalLM.from_pretrained(model_name)\n",
    "    tokenizer = transformers.GPTNeoXTokenizerFast.from_pretrained(model_name)\n",
    "    model.eval()\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     model, tokenizer = load_model('wellecks/llmstep-mathlib4-pythia2.8b')\n",
    "\n",
    "#     evaluation_theorems = [\n",
    "#         \"\"\"theorem thm1 (a b c : Nat) : a + b = c → a ≤ c := by {}\"\"\",\n",
    "#         \"\"\"theorem thm2 (x y : ℝ) : x < y → 0 < y - x := by {}\"\"\",\n",
    "#         \"\"\"theorem thm3 (n : Nat) : n ≥ 0 := by {}\"\"\",\n",
    "#         \"\"\"theorem thm4 (x y z : ℝ) : x ≤ y → y ≤ z → x ≤ z := by {}\"\"\",\n",
    "#         \"\"\"theorem thm5 (m n : Nat) (h : m.coprime n) : m.gcd n = 1 := by {}\"\"\",\n",
    "#         \"\"\"theorem thm6: r ⊆ s → s ⊆ t → r ⊆ t := by {}\"\"\",\n",
    "#         \"\"\"theorem thm7 (f : ℕ → ℕ) : Monotone f → ∀ n, f n ≤ f (n + 1) := by {}\"\"\",\n",
    "#         \"\"\"theorem thm8 (c : ℝ) : Injective fun x => x + c := by {}\"\"\",\n",
    "#         \"\"\"theorem thm9 (p q : Prop) : (p ∧ q) → ¬(¬p ∨ ¬q) := by {}\"\"\",\n",
    "#         \"\"\"theorem thm10 (A B : Set ℕ) : A ⊆ B → ∀ n, n ∈ A → n ∈ B := by {}\"\"\",\n",
    "#         \"\"\"theorem thm11 (injg : Injective g) (injf : Injective f) : Injective fun x => g (f x) := by {}\"\"\",\n",
    "#         \"\"\"theorem thm12 (a b : ℕ) (h : a ≤ b) : a * (a + 1) ≤ b * (b + 1) := by {}\"\"\",\n",
    "#         \"\"\"theorem thm13 (a b : ℕ) (h : a ≠ b) : a * 2 ≠ b * 2 := by {}\"\"\",\n",
    "#     ]\n",
    "        \n",
    "#     # Shared header for the theorems above\n",
    "#     header = \"\"\"import Mathlib.Data.Nat.Factorization.Basic\n",
    "#     import Mathlib.Data.Nat.Prime\n",
    "#     import Mathlib.Data.Real.Basic\n",
    "    \n",
    "#     open BigOperators\n",
    "#     open Function\n",
    "#     variable {α : Type _} (r s t : Set α)\n",
    "    \n",
    "#     \"\"\"\n",
    "\n",
    "#     results = []\n",
    "#     for theorem in evaluation_theorems:\n",
    "#         result = best_first_search(\n",
    "#             model, tokenizer, header, theorem, \n",
    "#             max_iters=32,\n",
    "#             temperatures=[0.5],\n",
    "#             num_samples=16\n",
    "#         )\n",
    "#         print(result)\n",
    "#         print('\\n-----\\n')\n",
    "#         results.append(result)\n",
    "\n",
    "#     print(len([x for x in results if x['success']])/len(results))\n",
    "#     _save(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4e5db185-cc22-4c7b-ad49-2eb85c8a1557",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformers.set_seed(43)\n",
    "\n",
    "def prove_simple(model, tokenizer, header, theorem_statement, search_budget):\n",
    "    success = False\n",
    "\n",
    "    code = header + theorem_statement\n",
    "    steps = []\n",
    "    proof = ''\n",
    "\n",
    "    for i in range(search_budget):\n",
    "        print(\"== Current (%d): \" % i, theorem_statement[:-3] + '\\n' + proof, sep='\\n')\n",
    "\n",
    "        # Run the code (header + proof-so-far)\n",
    "        state = run_code(code)\n",
    "        \n",
    "        # Stop if the proof is complete.\n",
    "        if is_done(state):\n",
    "            success = True\n",
    "            break\n",
    "\n",
    "        # Get the new state.\n",
    "        goal_candidate = get_goal(state)\n",
    "        if goal_candidate is None:\n",
    "            print(\"-- Error: backtracking\")\n",
    "            steps = steps[:-1]\n",
    "        else:\n",
    "            goal = goal_candidate\n",
    "\n",
    "        print(\"-- Goal: \", goal, sep='\\n')\n",
    "\n",
    "        # Generate a next-step\n",
    "        prompt = f\"[GOAL]{goal}[PROOFSTEP]\"\n",
    "        texts, _= generate(prompt, model, tokenizer, temperatures=[0.5], num_samples=1)\n",
    "        step = parse_step(texts[0])\n",
    "\n",
    "        # Add the next-step to the proof-so-far\n",
    "        steps.append(step)\n",
    "        proof = '\\n'.join(steps)\n",
    "        code = header + theorem_statement.replace(\" {}\", \"\") + '\\n' + proof\n",
    "        print()\n",
    "\n",
    "    if success:\n",
    "        print(\"\\nSUCCESS!\")\n",
    "    else:\n",
    "        print(\"\\nFAILED\")\n",
    "    \n",
    "    print(theorem_statement.replace(\" {}\", \"\"))\n",
    "    print ('  ' + proof.replace('\\n', '\\n  '))\n",
    "    \n",
    "    return {'theorem_statement': theorem_statement, 'proof': proof, 'success': success}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "64f1151f-9fc6-4696-8146-696b6f68c1db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Current (0): \n",
      "theorem thm1 (a b c : Nat) : a + b = c → a ≤ c := by\n",
      "\n",
      "-- Goal: \n",
      "a b c : ℕ\n",
      "⊢ a + b = c → a ≤ c\n",
      "\n",
      "== Current (1): \n",
      "theorem thm1 (a b c : Nat) : a + b = c → a ≤ c := by\n",
      "rintro rfl\n",
      "-- Goal: \n",
      "a b : ℕ\n",
      "⊢ a ≤ a + b\n",
      "\n",
      "== Current (2): \n",
      "theorem thm1 (a b c : Nat) : a + b = c → a ≤ c := by\n",
      "rintro rfl\n",
      "exact le_add_left _ _\n",
      "-- Error: backtracking\n",
      "-- Goal: \n",
      "a b : ℕ\n",
      "⊢ a ≤ a + b\n",
      "\n",
      "== Current (3): \n",
      "theorem thm1 (a b c : Nat) : a + b = c → a ≤ c := by\n",
      "rintro rfl\n",
      "apply Nat.le_add_right\n",
      "\n",
      "SUCCESS!\n",
      "theorem thm1 (a b c : Nat) : a + b = c → a ≤ c := by\n",
      "  rintro rfl\n",
      "  apply Nat.le_add_right\n"
     ]
    }
   ],
   "source": [
    "header = \"\"\"\n",
    "import Mathlib.Data.Nat.Prime\n",
    "\n",
    "\"\"\"\n",
    "theorem_statement = \"\"\"theorem thm1 (a b c : Nat) : a + b = c → a ≤ c := by {}\"\"\"\n",
    "\n",
    "out = prove_simple(\n",
    "    model, \n",
    "    tokenizer,\n",
    "    header, \n",
    "    theorem_statement, \n",
    "    search_budget=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "636f77ca-a807-44c7-8b21-ab1be283350a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/localscratch/hsun409/anaconda3/envs/jepa/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.277\trw [Nat.coprime, gcd_comm] at h\n",
      "-0.279\trw [← h.gcd_eq_one]\n",
      "-0.302\trw [← h.gcd_eq_one, Nat.gcd_comm]\n",
      "-0.326\trw [← h.gcd_eq_one, gcd_comm]\n",
      "-0.367\trw [← h.gcd_eq_right_iff_dvd]\n"
     ]
    }
   ],
   "source": [
    "prompt = '[GOAL]m n : ℕ\\nh : Nat.coprime m n\\n⊢ Nat.gcd m n = 1[PROOFSTEP]'\n",
    "\n",
    "texts, scores = generate(prompt, model, tokenizer, temperatures=[0.0], num_samples=5)\n",
    "\n",
    "for text, score in zip(texts, scores):\n",
    "    print('%.3f' % score, text, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "92e51dc2-fc7c-4424-b6a7-2f444f8fbcce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- current:\n",
      "\ttheorem thm1 (a b c : Nat) : a + b = c → a ≤ c := by \n",
      "\t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:04<00:00,  1.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- type-checked candidates:\n",
      "\t(-0.066) rintro rfl\n",
      "\t(-0.230) rintro ⟨d, rfl⟩\n",
      "\t(-0.301) rintro ⟨⟨a, rfl⟩, b, rfl⟩\n",
      "\t(-0.307) rintro ⟨rfl, rfl⟩\n",
      "--- current:\n",
      "\ttheorem thm1 (a b c : Nat) : a + b = c → a ≤ c := by \n",
      "\trintro rfl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:05<00:00,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- type-checked candidates:\n",
      "\t(-0.109) apply Nat.le_add_right\n",
      "\t(-0.173) exact Nat.le_add_right _ _\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'theorem_statement': 'theorem thm1 (a b c : Nat) : a + b = c → a ≤ c := by {}',\n",
       " 'proof': ['rintro rfl', 'apply Nat.le_add_right'],\n",
       " 'state': {'env': 0},\n",
       " 'score': 0.17478330433368683,\n",
       " 'success': True}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_first_search(\n",
    "    model, tokenizer, header, theorem_statement, \n",
    "    max_iters=32,\n",
    "    num_samples=4,\n",
    "    temperatures=[0.0],\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d106c3-bfba-4eed-ad63-dd89e53870f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
